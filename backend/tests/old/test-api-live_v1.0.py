#!/usr/bin/env python3
"""
Script de conversation naturelle avec Gemini Live API
Utilise le microphone pour l'input et les haut-parleurs pour l'output
Permet des interruptions naturelles et une conversation fluide

PrÃ©requis:
- Compte Google Cloud avec Vertex AI activÃ©
- Authentication Google Cloud (gcloud auth application-default login)
- Installation des dÃ©pendances (voir instructions ci-dessous)

Installation des dÃ©pendances:
# Sur Ubuntu
sudo apt-get install portaudio19-dev

# Sur macOS  
brew install portaudio

pip install google-genai pyaudio colorama google-auth google-cloud-core

# Authentication
gcloud auth application-default login
"""

import asyncio
import pyaudio
import signal
import sys
import os
import logging
import traceback
from typing import Optional
from colorama import Fore, Style, init
from google import genai
from google.genai import types
import json
from datetime import datetime

# Initialisation de colorama pour les couleurs dans le terminal
init(autoreset=True)

# Configuration du logging dÃ©taillÃ©
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('gemini_live_debug.log')
    ]
)
logger = logging.getLogger(__name__)

def log_debug(message: str, data: any = None):
    """Log de debug avec timestamp et donnÃ©es optionnelles"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    print(f"{Fore.CYAN}[DEBUG {timestamp}] {message}")
    logger.debug(f"{message} - Data: {data if data else 'None'}")

def log_info(message: str):
    """Log d'information"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    print(f"{Fore.GREEN}[INFO {timestamp}] {message}")
    logger.info(message)

def log_error(message: str, error: Exception = None):
    """Log d'erreur avec traceback"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    print(f"{Fore.RED}[ERROR {timestamp}] {message}")
    logger.error(f"{message} - Error: {error}")
    if error:
        logger.error(traceback.format_exc())

# Configuration du projet (variables en dur pour dÃ©veloppement local)
GOOGLE_CLOUD_PROJECT = "animemate-ddb62"
GOOGLE_CLOUD_LOCATION = "europe-west1"
# Configuration audio - OptimisÃ©e pour fluiditÃ©
CHUNK = 2048  # AugmentÃ© pour plus de fluiditÃ©
FORMAT = pyaudio.paInt16
CHANNELS = 1
INPUT_RATE = 16000
OUTPUT_RATE = 24000
# ModÃ¨le Live API disponible publiquement
MODEL = 'gemini-live-2.5-flash-preview-native-audio'  # Version public preview

# Variables globales pour la gestion propre de l'arrÃªt
running = True
session = None

class ConversationManager:
    def __init__(self):
        log_info("ğŸ”§ Initialisation du ConversationManager")
        
        # Test de l'authentification (optionnel)
        auth_method = self._setup_authentication()
        
        log_debug("ğŸ“¡ CrÃ©ation du client avec authentification")
        try:
            if auth_method == "api_key":
                log_info("ğŸ”‘ Utilisation de la clÃ© API pour l'authentification")
                self.client = genai.Client(
                    api_key=GEMINI_API_KEY,
                    # Pour l'API key, on utilise le client standard (pas Vertex AI)
                )
            else:
                log_info("â˜ï¸ Utilisation de Vertex AI avec authentification par dÃ©faut")
                self.client = genai.Client(
                    vertexai=True,
                    project=GOOGLE_CLOUD_PROJECT,
                    location=GOOGLE_CLOUD_LOCATION,
                )
            log_info("âœ… Client crÃ©Ã© avec succÃ¨s")
        except Exception as e:
            log_error("âŒ Erreur crÃ©ation client", e)
            raise
            raise
        
        log_debug("ğŸ›ï¸ Configuration de la session Live API")
        # Configuration de la session avec voix franÃ§aise et Affective Dialog
        # Note: Le modÃ¨le native-audio ne supporte que response_modalities=["AUDIO"]
        self.config = types.LiveConnectConfig(
            response_modalities=["AUDIO"],
            speech_config=types.SpeechConfig(
                voice_config=types.VoiceConfig(
                    prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name="Charon",  # Voix masculine franÃ§aise
                        # Alternatives: "Coral" (fÃ©minine), "River" (neutre)
                    )
                ),
                language_code="fr-FR",
            ),
            input_audio_transcription=types.AudioTranscriptionConfig(),
            output_audio_transcription=types.AudioTranscriptionConfig(),
            system_instruction=types.Content(
                parts=[types.Part(text="""Tu es un assistant IA conversationnel en franÃ§ais avec une personnalitÃ© expressive et adaptative. 
                Adapte ton ton et style de rÃ©ponse selon l'Ã©motion et le ton de l'utilisateur :
                - Si l'utilisateur semble joyeux, sois enthousiaste
                - Si l'utilisateur semble inquiet, sois rassurant et calme
                - Si l'utilisateur semble pressÃ©, sois concis et efficace
                - Si l'utilisateur semble dÃ©tendu, sois conversationnel
                
                RÃ©ponds de maniÃ¨re naturelle, claire et avec de l'Ã©motion dans la voix.
                Utilise des expressions franÃ§aises appropriÃ©es au contexte Ã©motionnel.
                RÃ‰PONDS TOUJOURS EN FRANÃ‡AIS DE MANIÃˆRE NATURELLE ET EXPRESSIVE.""")], 
                role='user'
            ),
            # Configuration pour des rÃ©ponses plus rapides et fluides
            realtime_input_config=types.RealtimeInputConfig(
                automatic_activity_detection=types.AutomaticActivityDetection(
                    disabled=False,
                    start_of_speech_sensitivity=types.StartSensitivity.START_SENSITIVITY_HIGH,
                    end_of_speech_sensitivity=types.EndSensitivity.END_SENSITIVITY_HIGH,  # Plus sensible
                    prefix_padding_ms=100,  # Plus de padding
                    silence_duration_ms=500,  # Moins de silence requis
                )
            ),
            # âœ¨ AFFECTIVE DIALOG ACTIVÃ‰ âœ¨
            enable_affective_dialog=True,  # Adaptation du style selon le ton vocal
        )
        log_info("âœ… Configuration Live API native-audio avec Affective Dialog crÃ©Ã©e")
        
        log_debug("ğŸ”Š Initialisation interface audio PyAudio")
        try:
            self.audio_interface = pyaudio.PyAudio()
            log_info("âœ… Interface audio initialisÃ©e")
            
            # Affichage des devices audio disponibles
            self._list_audio_devices()
            
        except Exception as e:
            log_error("âŒ Erreur initialisation audio", e)
            raise
    
    def _setup_authentication(self):
        """Configuration de l'authentification (Vertex AI ou API Key)"""
        log_debug("ğŸ” Configuration de l'authentification")
        
        # PrioritÃ© 1: Essayer l'authentification Vertex AI
        try:
            from google.auth import default
            credentials, project = default()
            log_info(f"âœ… Authentification Vertex AI OK - Projet: {project}")
            
            if project != GOOGLE_CLOUD_PROJECT:
                log_debug(f"âš ï¸ Projet dÃ©tectÃ© ({project}) diffÃ©rent du configurÃ© ({GOOGLE_CLOUD_PROJECT})")
                
            return "vertex_ai"
            
        except Exception as e:
            log_debug(f"âš ï¸ Authentification Vertex AI Ã©chouÃ©e: {e}")
            
            # PrioritÃ© 2: Utiliser la clÃ© API
            if GEMINI_API_KEY:
                log_info("ğŸ”‘ Basculement sur l'authentification par clÃ© API")
                return "api_key"
            else:
                log_error("âŒ Aucune mÃ©thode d'authentification disponible")
                print(f"{Fore.RED}ğŸ”§ Solutions possibles:")
                print(f"{Fore.YELLOW}   1. Authentification Vertex AI:")
                print(f"{Fore.YELLOW}      gcloud auth application-default login")
                print(f"{Fore.YELLOW}      gcloud config set project {GOOGLE_CLOUD_PROJECT}")
                print(f"{Fore.YELLOW}   2. Ou vÃ©rifiez votre clÃ© API Gemini")
                raise Exception("Aucune authentification disponible")
    
    def _list_audio_devices(self):
        """Liste les devices audio disponibles"""
        log_debug("ğŸ¤ Liste des devices audio disponibles:")
        try:
            info = self.audio_interface.get_host_api_info_by_index(0)
            log_debug(f"API Audio: {info.get('name')}")
            
            for i in range(self.audio_interface.get_device_count()):
                device_info = self.audio_interface.get_device_info_by_index(i)
                log_debug(f"  Device {i}: {device_info.get('name')} "
                         f"(In: {device_info.get('maxInputChannels')}, "
                         f"Out: {device_info.get('maxOutputChannels')})")
                         
        except Exception as e:
            log_error("âš ï¸ Erreur listage devices audio", e)
        
    async def start_conversation(self):
        """DÃ©marre une conversation interactive avec Gemini Live API"""
        log_info("ğŸš€ DÃ©marrage de la conversation avec Gemini Live API")
        print(f"{Fore.GREEN}ğŸ¤ DÃ©marrage de la conversation avec Gemini Live API...")
        print(f"{Fore.YELLOW}ğŸ“ Parlez naturellement, l'IA vous rÃ©pondra vocalement")
        print(f"{Fore.YELLOW}ğŸ›‘ Ctrl+C pour arrÃªter la conversation")
        print(f"{Fore.CYAN}{'='*60}")
        
        global session
        try:
            log_debug("ğŸ“¡ Tentative de connexion Ã  Live API")
            log_debug(f"ModÃ¨le: {MODEL}")
            log_debug(f"Config: {self.config}")
            
            # Note: Le modÃ¨le Live API pourrait ne pas Ãªtre disponible avec la clÃ© API standard
            # Dans ce cas, utilisez l'authentification Vertex AI
            async with self.client.aio.live.connect(model=MODEL, config=self.config) as session:
                log_info("âœ… Connexion Live API Ã©tablie")
                
                # DÃ©marrage des tÃ¢ches asynchrones
                log_debug("ğŸ”„ DÃ©marrage des tÃ¢ches audio")
                send_task = asyncio.create_task(self._send_audio())
                receive_task = asyncio.create_task(self._receive_audio())
                
                log_info("ğŸ¤ TÃ¢ches audio dÃ©marrÃ©es - Conversation active")
                # Attendre que les deux tÃ¢ches se terminent
                await asyncio.gather(send_task, receive_task)
                
        except Exception as e:
            log_error("âŒ Erreur lors de la connexion Live API", e)
            print(f"{Fore.RED}âŒ Erreur lors de la connexion: {e}")
            
            # Diagnostic dÃ©taillÃ©
            print(f"{Fore.YELLOW}ğŸ” Diagnostic:")
            print(f"  - Projet: {GOOGLE_CLOUD_PROJECT}")
            print(f"  - RÃ©gion: {GOOGLE_CLOUD_LOCATION}")
            print(f"  - ModÃ¨le: {MODEL}")
            
            # Si erreur de modÃ¨le non trouvÃ©, suggÃ©rer des alternatives
            if "was not fo" in str(e) or "not found" in str(e).lower():
                print(f"{Fore.YELLOW}ğŸ’¡ Le modÃ¨le Live API n'est pas disponible dans votre projet.")
                print(f"{Fore.YELLOW}   Solutions possibles:")
                print(f"{Fore.YELLOW}   1. Demander l'accÃ¨s au modÃ¨le privÃ© 'gemini-live-2.5-flash'")
                print(f"{Fore.YELLOW}   2. Contacter votre Ã©quipe Google Cloud pour l'activation")
                print(f"{Fore.YELLOW}   3. VÃ©rifier les quotas et limites de votre projet")
            
            raise
            
    async def _send_audio(self):
        """Capture et envoie l'audio du microphone"""
        stream = None
        try:
            log_debug("ğŸ¤ Ouverture du stream d'entrÃ©e audio")
            stream = self.audio_interface.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=INPUT_RATE,
                input=True,
                frames_per_buffer=CHUNK
            )
            log_info("âœ… Stream microphone ouvert")
            
            print(f"{Fore.GREEN}ğŸ¤ Microphone activÃ© - Vous pouvez commencer Ã  parler...")
            
            chunk_count = 0
            while running:
                try:
                    frame = stream.read(CHUNK, exception_on_overflow=False)
                    chunk_count += 1
                    
                    if chunk_count % 100 == 0:  # Log tous les 100 chunks
                        log_debug(f"ğŸ“Š Chunk audio #{chunk_count} envoyÃ© ({len(frame)} bytes)")
                    
                    await session.send_realtime_input(
                        media=types.Blob(data=frame, mime_type="audio/pcm;rate=16000")
                    )
                    await asyncio.sleep(0.001)  # Petite pause pour Ã©viter la surcharge
                    
                except Exception as e:
                    if running:
                        log_error(f"âš ï¸ Erreur d'envoi audio chunk #{chunk_count}", e)
                    break
                    
        except Exception as e:
            log_error("âŒ Erreur ouverture microphone", e)
        finally:
            if stream:
                log_debug("ğŸ” Fermeture du stream microphone")
                stream.stop_stream()
                stream.close()
                log_info("âœ… Stream microphone fermÃ©")
                
    async def _receive_audio(self):
        """ReÃ§oit et joue l'audio de rÃ©ponse"""
        output_stream = None
        message_count = 0
        audio_buffer = bytearray()  # Buffer pour lisser l'audio
        
        try:
            log_debug("ğŸ”Š Ouverture du stream de sortie audio")
            output_stream = self.audio_interface.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=OUTPUT_RATE,
                output=True,
                frames_per_buffer=CHUNK
            )
            log_info("âœ… Stream sortie audio ouvert")
            
            log_debug("ğŸ‘‚ Ã‰coute des messages Live API")
            async for message in session.receive():
                if not running:
                    break
                    
                message_count += 1
                log_debug(f"ğŸ“¨ Message #{message_count} reÃ§u", {
                    'type': type(message).__name__,
                    'has_server_content': hasattr(message, 'server_content') and message.server_content is not None
                })
                
                # Affichage des transcriptions
                if hasattr(message, 'server_content') and message.server_content:
                    if message.server_content.input_transcription:
                        user_text = message.server_content.input_transcription.text
                        # Protection contre les valeurs None
                        if user_text and user_text.strip():
                            log_info(f"ğŸ—£ï¸ Transcription utilisateur: {user_text}")
                            print(f"{Fore.BLUE}ğŸ‘¤ Vous: {Style.BRIGHT}{user_text}")
                            
                    if message.server_content.output_transcription:
                        ai_text = message.server_content.output_transcription.text
                        # Protection contre les valeurs None
                        if ai_text and ai_text.strip():
                            log_info(f"ğŸ¤– Transcription Gemini: {ai_text}")
                            print(f"{Fore.MAGENTA}ğŸ¤– Gemini: {Style.BRIGHT}{ai_text}")
                            
                    # Gestion des interruptions - Vider buffer et continuer
                    if message.server_content.interrupted:
                        log_info("â¸ï¸ Interruption dÃ©tectÃ©e - Vidage du buffer, conversation continue")
                        print(f"{Fore.YELLOW}â¸ï¸  Interruption dÃ©tectÃ©e - PrÃªt pour nouvelle question")
                        # Vider le buffer mais ne pas fermer le stream
                        audio_buffer.clear()
                        # Continuer la boucle pour permettre une nouvelle conversation
                        continue
                            
                    # Lecture de l'audio de rÃ©ponse avec buffering
                    if message.server_content.model_turn:
                        log_debug("ğŸµ RÃ©ception de donnÃ©es audio")
                        for part_idx, part in enumerate(message.server_content.model_turn.parts):
                            if part.inline_data and part.inline_data.data:
                                try:
                                    audio_size = len(part.inline_data.data)
                                    log_debug(f"ğŸ”Š Buffering audio part #{part_idx} ({audio_size} bytes)")
                                    
                                    # Ajouter au buffer
                                    audio_buffer.extend(part.inline_data.data)
                                    
                                    # Jouer quand on a assez de donnÃ©es (au moins 2 chunks)
                                    min_buffer_size = CHUNK * 4  # 4 chunks minimum pour fluiditÃ©
                                    while len(audio_buffer) >= min_buffer_size:
                                        # Extraire un chunk du buffer
                                        chunk_to_play = bytes(audio_buffer[:min_buffer_size])
                                        audio_buffer = audio_buffer[min_buffer_size:]
                                        
                                        # Jouer le chunk
                                        if output_stream and running:
                                            output_stream.write(chunk_to_play)
                                            await asyncio.sleep(0.001)
                                        
                                except Exception as e:
                                    if running:
                                        log_error(f"âš ï¸ Erreur lecture audio part #{part_idx}", e)
                        
                        # Jouer le reste du buffer Ã  la fin
                        if len(audio_buffer) > 0 and output_stream and running:
                            try:
                                # ComplÃ©ter avec du silence si nÃ©cessaire
                                while len(audio_buffer) % 2 != 0:
                                    audio_buffer.append(0)
                                output_stream.write(bytes(audio_buffer))
                                audio_buffer.clear()
                            except Exception as e:
                                if running:
                                    log_error("âš ï¸ Erreur lecture buffer final", e)
                        
        except Exception as e:
            if running:
                log_error("âŒ Erreur rÃ©ception audio", e)
        finally:
            # Jouer le buffer restant avant fermeture
            if len(audio_buffer) > 0 and output_stream:
                try:
                    while len(audio_buffer) % 2 != 0:
                        audio_buffer.append(0)
                    output_stream.write(bytes(audio_buffer))
                except:
                    pass
                    
            if output_stream:
                log_debug("ğŸ” Fermeture du stream sortie audio")
                output_stream.stop_stream()
                output_stream.close()
                log_info("âœ… Stream sortie audio fermÃ©")
                
    def cleanup(self):
        """Nettoyage des ressources"""
        try:
            self.audio_interface.terminate()
            print(f"{Fore.GREEN}âœ… Ressources audio libÃ©rÃ©es")
        except Exception as e:
            print(f"{Fore.RED}âš ï¸  Erreur lors du nettoyage: {e}")

def signal_handler(signum, frame):
    """Gestionnaire pour arrÃªt propre avec Ctrl+C"""
    global running
    print(f"\n{Fore.YELLOW}ğŸ›‘ ArrÃªt de la conversation...")
    running = False

def main():
    """Fonction principale"""
    # Configuration directe (variables en dur pour dÃ©veloppement local)
    project_id = "animemate-ddb62"
    location = "europe-west1"
    
    print(f"{Fore.GREEN}âœ… Configuration chargÃ©e:")
    print(f"{Fore.GREEN}   GOOGLE_CLOUD_PROJECT={project_id}")
    print(f"{Fore.GREEN}   GOOGLE_CLOUD_LOCATION={location}")
    
    # Configuration du gestionnaire de signal pour Ctrl+C
    signal.signal(signal.SIGINT, signal_handler)
    
    # Initialisation et dÃ©marrage
    conversation = ConversationManager()
    
    try:
        print(f"{Fore.CYAN}ğŸš€ Initialisation de la conversation avec Gemini Live API")
        print(f"{Fore.CYAN}ğŸ“ Projet: {project_id}")
        print(f"{Fore.CYAN}ğŸŒ RÃ©gion: {location}")
        
        asyncio.run(conversation.start_conversation())
        
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}ğŸ›‘ Conversation interrompue par l'utilisateur")
    except Exception as e:
        print(f"{Fore.RED}âŒ Erreur inattendue: {e}")
    finally:
        conversation.cleanup()
        print(f"{Fore.GREEN}ğŸ‘‹ Au revoir !")

if __name__ == "__main__":
    main()