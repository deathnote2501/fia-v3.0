#!/usr/bin/env python3
"""
Script de conversation naturelle avec Gemini Live API
Utilise le microphone pour l'input et les haut-parleurs pour l'output
Permet des interruptions naturelles et une conversation fluide

Pr√©requis:
- Compte Google Cloud avec Vertex AI activ√©
- Authentication Google Cloud (gcloud auth application-default login)
- Installation des d√©pendances (voir instructions ci-dessous)

Installation des d√©pendances:
# Sur Ubuntu
sudo apt-get install portaudio19-dev

# Sur macOS  
brew install portaudio

pip install google-genai pyaudio colorama google-auth google-cloud-core

# Authentication
gcloud auth application-default login
"""

import asyncio
import pyaudio
import signal
import sys
import os
import logging
import traceback
from typing import Optional
from colorama import Fore, Style, init
from google import genai
from google.genai import types
import json
from datetime import datetime

# Initialisation de colorama pour les couleurs dans le terminal
init(autoreset=True)

# Configuration du logging d√©taill√©
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('gemini_live_debug.log')
    ]
)
logger = logging.getLogger(__name__)

def log_debug(message: str, data: any = None):
    """Log de debug avec timestamp et donn√©es optionnelles"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    print(f"{Fore.CYAN}[DEBUG {timestamp}] {message}")
    logger.debug(f"{message} - Data: {data if data else 'None'}")

def log_info(message: str):
    """Log d'information"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    print(f"{Fore.GREEN}[INFO {timestamp}] {message}")
    logger.info(message)

def log_error(message: str, error: Exception = None):
    """Log d'erreur avec traceback"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    print(f"{Fore.RED}[ERROR {timestamp}] {message}")
    logger.error(f"{message} - Error: {error}")
    if error:
        logger.error(traceback.format_exc())

# Configuration du projet (variables en dur pour d√©veloppement local)
GOOGLE_CLOUD_PROJECT = "animemate-ddb62"
GOOGLE_CLOUD_LOCATION = "europe-west1"
# Configuration audio - Optimis√©e pour fluidit√©
CHUNK = 2048  # Augment√© pour plus de fluidit√©
FORMAT = pyaudio.paInt16
CHANNELS = 1
INPUT_RATE = 16000
OUTPUT_RATE = 24000
# Mod√®le Live API disponible publiquement
MODEL = 'gemini-live-2.5-flash-preview-native-audio'  # Version public preview

# Variables globales pour la gestion propre de l'arr√™t
running = True
session = None

class ConversationManager:
    def __init__(self):
        log_info("üîß Initialisation du ConversationManager")
        
        # Test de l'authentification (optionnel)
        auth_method = self._setup_authentication()
        
        log_debug("üì° Cr√©ation du client avec authentification")
        try:
            if auth_method == "api_key":
                log_info("üîë Utilisation de la cl√© API pour l'authentification")
                self.client = genai.Client(
                    api_key=GEMINI_API_KEY,
                    # Pour l'API key, on utilise le client standard (pas Vertex AI)
                )
            else:
                log_info("‚òÅÔ∏è Utilisation de Vertex AI avec authentification par d√©faut")
                self.client = genai.Client(
                    vertexai=True,
                    project=GOOGLE_CLOUD_PROJECT,
                    location=GOOGLE_CLOUD_LOCATION,
                )
            log_info("‚úÖ Client cr√©√© avec succ√®s")
        except Exception as e:
            log_error("‚ùå Erreur cr√©ation client", e)
            raise
            raise
        
        log_debug("üéõÔ∏è Configuration de la session Live API")
        # Configuration de la session avec voix fran√ßaise et Affective Dialog
        # Note: Le mod√®le native-audio ne supporte que response_modalities=["AUDIO"]
        self.config = types.LiveConnectConfig(
            response_modalities=["AUDIO"],
            speech_config=types.SpeechConfig(
                voice_config=types.VoiceConfig(
                    prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name="Charon",  # Voix masculine fran√ßaise
                        # Alternatives: "Coral" (f√©minine), "River" (neutre)
                    )
                ),
                language_code="fr-FR",
            ),
            input_audio_transcription=types.AudioTranscriptionConfig(),
            output_audio_transcription=types.AudioTranscriptionConfig(),
            system_instruction=types.Content(
                parts=[types.Part(text="""Tu es un assistant IA conversationnel en fran√ßais avec une personnalit√© expressive et adaptative. 
                Adapte ton ton et style de r√©ponse selon l'√©motion et le ton de l'utilisateur :
                - Si l'utilisateur semble joyeux, sois enthousiaste
                - Si l'utilisateur semble inquiet, sois rassurant et calme
                - Si l'utilisateur semble press√©, sois concis et efficace
                - Si l'utilisateur semble d√©tendu, sois conversationnel
                
                R√©ponds de mani√®re naturelle, claire et avec de l'√©motion dans la voix.
                Utilise des expressions fran√ßaises appropri√©es au contexte √©motionnel.
                R√âPONDS TOUJOURS EN FRAN√áAIS DE MANI√àRE NATURELLE ET EXPRESSIVE.""")], 
                role='user'
            ),
            # Configuration pour des r√©ponses plus rapides et fluides
            realtime_input_config=types.RealtimeInputConfig(
                automatic_activity_detection=types.AutomaticActivityDetection(
                    disabled=False,
                    start_of_speech_sensitivity=types.StartSensitivity.START_SENSITIVITY_HIGH,
                    end_of_speech_sensitivity=types.EndSensitivity.END_SENSITIVITY_HIGH,  # Plus sensible
                    prefix_padding_ms=100,  # Plus de padding
                    silence_duration_ms=500,  # Moins de silence requis
                )
            ),
            # ‚ú® AFFECTIVE DIALOG ACTIV√â ‚ú®
            enable_affective_dialog=True,  # Adaptation du style selon le ton vocal
        )
        log_info("‚úÖ Configuration Live API native-audio avec Affective Dialog cr√©√©e")
        
        log_debug("üîä Initialisation interface audio PyAudio")
        try:
            self.audio_interface = pyaudio.PyAudio()
            log_info("‚úÖ Interface audio initialis√©e")
            
            # Affichage des devices audio disponibles
            self._list_audio_devices()
            
        except Exception as e:
            log_error("‚ùå Erreur initialisation audio", e)
            raise
    
    def _setup_authentication(self):
        """Configuration de l'authentification (Vertex AI ou API Key)"""
        log_debug("üîê Configuration de l'authentification")
        
        # Priorit√© 1: Essayer l'authentification Vertex AI
        try:
            from google.auth import default
            credentials, project = default()
            log_info(f"‚úÖ Authentification Vertex AI OK - Projet: {project}")
            
            if project != GOOGLE_CLOUD_PROJECT:
                log_debug(f"‚ö†Ô∏è Projet d√©tect√© ({project}) diff√©rent du configur√© ({GOOGLE_CLOUD_PROJECT})")
                
            return "vertex_ai"
            
        except Exception as e:
            log_debug(f"‚ö†Ô∏è Authentification Vertex AI √©chou√©e: {e}")
            
            # Priorit√© 2: Utiliser la cl√© API
            if GEMINI_API_KEY:
                log_info("üîë Basculement sur l'authentification par cl√© API")
                return "api_key"
            else:
                log_error("‚ùå Aucune m√©thode d'authentification disponible")
                print(f"{Fore.RED}üîß Solutions possibles:")
                print(f"{Fore.YELLOW}   1. Authentification Vertex AI:")
                print(f"{Fore.YELLOW}      gcloud auth application-default login")
                print(f"{Fore.YELLOW}      gcloud config set project {GOOGLE_CLOUD_PROJECT}")
                print(f"{Fore.YELLOW}   2. Ou v√©rifiez votre cl√© API Gemini")
                raise Exception("Aucune authentification disponible")
    
    def _list_audio_devices(self):
        """Liste les devices audio disponibles"""
        log_debug("üé§ Liste des devices audio disponibles:")
        try:
            info = self.audio_interface.get_host_api_info_by_index(0)
            log_debug(f"API Audio: {info.get('name')}")
            
            for i in range(self.audio_interface.get_device_count()):
                device_info = self.audio_interface.get_device_info_by_index(i)
                log_debug(f"  Device {i}: {device_info.get('name')} "
                         f"(In: {device_info.get('maxInputChannels')}, "
                         f"Out: {device_info.get('maxOutputChannels')})")
                         
        except Exception as e:
            log_error("‚ö†Ô∏è Erreur listage devices audio", e)
        
    async def start_conversation(self):
        """D√©marre une conversation interactive avec Gemini Live API"""
        log_info("üöÄ D√©marrage de la conversation avec Gemini Live API")
        print(f"{Fore.GREEN}üé§ D√©marrage de la conversation avec Gemini Live API...")
        print(f"{Fore.YELLOW}üìù Parlez naturellement, l'IA vous r√©pondra vocalement")
        print(f"{Fore.YELLOW}üõë Ctrl+C pour arr√™ter la conversation")
        print(f"{Fore.CYAN}{'='*60}")
        
        global session
        try:
            log_debug("üì° Tentative de connexion √† Live API")
            log_debug(f"Mod√®le: {MODEL}")
            log_debug(f"Config: {self.config}")
            
            # Note: Le mod√®le Live API pourrait ne pas √™tre disponible avec la cl√© API standard
            # Dans ce cas, utilisez l'authentification Vertex AI
            async with self.client.aio.live.connect(model=MODEL, config=self.config) as session:
                log_info("‚úÖ Connexion Live API √©tablie")
                
                # D√©marrage des t√¢ches asynchrones
                log_debug("üîÑ D√©marrage des t√¢ches audio")
                send_task = asyncio.create_task(self._send_audio())
                receive_task = asyncio.create_task(self._receive_audio())
                
                log_info("üé§ T√¢ches audio d√©marr√©es - Conversation active")
                # Attendre que les deux t√¢ches se terminent
                await asyncio.gather(send_task, receive_task)
                
        except Exception as e:
            log_error("‚ùå Erreur lors de la connexion Live API", e)
            print(f"{Fore.RED}‚ùå Erreur lors de la connexion: {e}")
            
            # Diagnostic d√©taill√©
            print(f"{Fore.YELLOW}üîç Diagnostic:")
            print(f"  - Projet: {GOOGLE_CLOUD_PROJECT}")
            print(f"  - R√©gion: {GOOGLE_CLOUD_LOCATION}")
            print(f"  - Mod√®le: {MODEL}")
            
            # Si erreur de mod√®le non trouv√©, sugg√©rer des alternatives
            if "was not fo" in str(e) or "not found" in str(e).lower():
                print(f"{Fore.YELLOW}üí° Le mod√®le Live API n'est pas disponible dans votre projet.")
                print(f"{Fore.YELLOW}   Solutions possibles:")
                print(f"{Fore.YELLOW}   1. Demander l'acc√®s au mod√®le priv√© 'gemini-live-2.5-flash'")
                print(f"{Fore.YELLOW}   2. Contacter votre √©quipe Google Cloud pour l'activation")
                print(f"{Fore.YELLOW}   3. V√©rifier les quotas et limites de votre projet")
            
            raise
            
    async def _send_audio(self):
        """Capture et envoie l'audio du microphone"""
        stream = None
        try:
            log_debug("üé§ Ouverture du stream d'entr√©e audio")
            stream = self.audio_interface.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=INPUT_RATE,
                input=True,
                frames_per_buffer=CHUNK
            )
            log_info("‚úÖ Stream microphone ouvert")
            
            print(f"{Fore.GREEN}üé§ Microphone activ√© - Vous pouvez commencer √† parler...")
            
            chunk_count = 0
            while running:
                try:
                    frame = stream.read(CHUNK, exception_on_overflow=False)
                    chunk_count += 1
                    
                    if chunk_count % 100 == 0:  # Log tous les 100 chunks
                        log_debug(f"üìä Chunk audio #{chunk_count} envoy√© ({len(frame)} bytes)")
                    
                    await session.send_realtime_input(
                        media=types.Blob(data=frame, mime_type="audio/pcm;rate=16000")
                    )
                    await asyncio.sleep(0.001)  # Petite pause pour √©viter la surcharge
                    
                except Exception as e:
                    if running:
                        log_error(f"‚ö†Ô∏è Erreur d'envoi audio chunk #{chunk_count}", e)
                    break
                    
        except Exception as e:
            log_error("‚ùå Erreur ouverture microphone", e)
        finally:
            if stream:
                log_debug("üîê Fermeture du stream microphone")
                stream.stop_stream()
                stream.close()
                log_info("‚úÖ Stream microphone ferm√©")
                
    async def _receive_audio(self):
        """Re√ßoit et joue l'audio de r√©ponse"""
        output_stream = None
        message_count = 0
        audio_buffer = bytearray()  # Buffer pour lisser l'audio
        
        try:
            log_debug("üîä Ouverture du stream de sortie audio")
            output_stream = self.audio_interface.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=OUTPUT_RATE,
                output=True,
                frames_per_buffer=CHUNK
            )
            log_info("‚úÖ Stream sortie audio ouvert")
            
            log_debug("üëÇ √âcoute des messages Live API")
            async for message in session.receive():
                if not running:
                    break
                    
                message_count += 1
                log_debug(f"üì® Message #{message_count} re√ßu", {
                    'type': type(message).__name__,
                    'has_server_content': hasattr(message, 'server_content') and message.server_content is not None
                })
                
                # Affichage des transcriptions
                if hasattr(message, 'server_content') and message.server_content:
                    if message.server_content.input_transcription:
                        user_text = message.server_content.input_transcription.text
                        # Protection contre les valeurs None
                        if user_text and user_text.strip():
                            log_info(f"üó£Ô∏è Transcription utilisateur: {user_text}")
                            print(f"{Fore.BLUE}üë§ Vous: {Style.BRIGHT}{user_text}")
                            
                    if message.server_content.output_transcription:
                        ai_text = message.server_content.output_transcription.text
                        # Protection contre les valeurs None
                        if ai_text and ai_text.strip():
                            log_info(f"ü§ñ Transcription Gemini: {ai_text}")
                            print(f"{Fore.MAGENTA}ü§ñ Gemini: {Style.BRIGHT}{ai_text}")
                            
                    # Gestion des interruptions - Vider buffer et continuer
                    if message.server_content.interrupted:
                        log_info("‚è∏Ô∏è Interruption d√©tect√©e - Vidage du buffer, conversation continue")
                        print(f"{Fore.YELLOW}‚è∏Ô∏è  Interruption d√©tect√©e - Pr√™t pour nouvelle question")
                        # Vider le buffer mais ne pas fermer le stream
                        audio_buffer.clear()
                        # Continuer la boucle pour permettre une nouvelle conversation
                        continue
                            
                    # Lecture de l'audio de r√©ponse avec buffering
                    if message.server_content.model_turn:
                        log_debug("üéµ R√©ception de donn√©es audio")
                        for part_idx, part in enumerate(message.server_content.model_turn.parts):
                            if part.inline_data and part.inline_data.data:
                                try:
                                    audio_size = len(part.inline_data.data)
                                    log_debug(f"üîä Buffering audio part #{part_idx} ({audio_size} bytes)")
                                    
                                    # Ajouter au buffer
                                    audio_buffer.extend(part.inline_data.data)
                                    
                                    # Jouer quand on a assez de donn√©es (au moins 2 chunks)
                                    min_buffer_size = CHUNK * 4  # 4 chunks minimum pour fluidit√©
                                    while len(audio_buffer) >= min_buffer_size:
                                        # Extraire un chunk du buffer
                                        chunk_to_play = bytes(audio_buffer[:min_buffer_size])
                                        audio_buffer = audio_buffer[min_buffer_size:]
                                        
                                        # Jouer le chunk
                                        if output_stream and running:
                                            output_stream.write(chunk_to_play)
                                            await asyncio.sleep(0.001)
                                        
                                except Exception as e:
                                    if running:
                                        log_error(f"‚ö†Ô∏è Erreur lecture audio part #{part_idx}", e)
                        
                        # Jouer le reste du buffer √† la fin
                        if len(audio_buffer) > 0 and output_stream and running:
                            try:
                                # Compl√©ter avec du silence si n√©cessaire
                                while len(audio_buffer) % 2 != 0:
                                    audio_buffer.append(0)
                                output_stream.write(bytes(audio_buffer))
                                audio_buffer.clear()
                            except Exception as e:
                                if running:
                                    log_error("‚ö†Ô∏è Erreur lecture buffer final", e)
                        
        except Exception as e:
            if running:
                log_error("‚ùå Erreur r√©ception audio", e)
        finally:
            # Jouer le buffer restant avant fermeture
            if len(audio_buffer) > 0 and output_stream:
                try:
                    while len(audio_buffer) % 2 != 0:
                        audio_buffer.append(0)
                    output_stream.write(bytes(audio_buffer))
                except:
                    pass
                    
            if output_stream:
                log_debug("üîê Fermeture du stream sortie audio")
                output_stream.stop_stream()
                output_stream.close()
                log_info("‚úÖ Stream sortie audio ferm√©")
                
    def cleanup(self):
        """Nettoyage des ressources"""
        try:
            self.audio_interface.terminate()
            print(f"{Fore.GREEN}‚úÖ Ressources audio lib√©r√©es")
        except Exception as e:
            print(f"{Fore.RED}‚ö†Ô∏è  Erreur lors du nettoyage: {e}")

def signal_handler(signum, frame):
    """Gestionnaire pour arr√™t propre avec Ctrl+C"""
    global running
    print(f"\n{Fore.YELLOW}üõë Arr√™t de la conversation...")
    running = False

def main():
    """Fonction principale"""
    # Configuration directe (variables en dur pour d√©veloppement local)
    project_id = "animemate-ddb62"
    location = "europe-west1"
    
    print(f"{Fore.GREEN}‚úÖ Configuration charg√©e:")
    print(f"{Fore.GREEN}   GOOGLE_CLOUD_PROJECT={project_id}")
    print(f"{Fore.GREEN}   GOOGLE_CLOUD_LOCATION={location}")
    
    # Configuration du gestionnaire de signal pour Ctrl+C
    signal.signal(signal.SIGINT, signal_handler)
    
    # Initialisation et d√©marrage
    conversation = ConversationManager()
    
    try:
        print(f"{Fore.CYAN}üöÄ Initialisation de la conversation avec Gemini Live API")
        print(f"{Fore.CYAN}üìç Projet: {project_id}")
        print(f"{Fore.CYAN}üåç R√©gion: {location}")
        
        asyncio.run(conversation.start_conversation())
        
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}üõë Conversation interrompue par l'utilisateur")
    except Exception as e:
        print(f"{Fore.RED}‚ùå Erreur inattendue: {e}")
    finally:
        conversation.cleanup()
        print(f"{Fore.GREEN}üëã Au revoir !")

if __name__ == "__main__":
    main()